{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Threading Tutorial\n",
    "\n",
    "This notebook is a tutorial for Julia's built-in multi-threading capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-threading Overview\n",
    "\n",
    "Multi-threading is a type of *shared memory* parallel programming.\n",
    "\n",
    "- It is a way to make use of all the cores available on your computer when you run computations.\n",
    "- All of these cores have access to the same memory (technically, they still have their own cache hierarchies)\n",
    "- Each core can be doing work separately\n",
    "\n",
    "**Exercise 0**: How many cores does your computer have? Look it up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "All of the multi-threading capabilities are handeled in Base Julia, specifically in a package `Base.Threads`.\n",
    "\n",
    "- You don't need to load this package since it's part of Base\n",
    "- You do need to write macros like `Threads.@spawn` unless you import these functions specifically\n",
    "\n",
    "Let's call `nthreads` to see how many threads we have available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, I only have one thread! Why is this?\n",
    "\n",
    "**We need to tell Julia how many threads it has available when it starts up**\n",
    "\n",
    "When you run a julia executable or interactive julia in the REPL, this can be done with\n",
    "\n",
    "```\n",
    "julia --threads=4\n",
    "```\n",
    "\n",
    "**Exercise 1**: Start an interactive Julia session with the `--threads` command, and run `Threads.nthreads()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-threading in a Jupyter Notebook\n",
    "\n",
    "In order to get multi-threading to work in a Jupyter notebook, you need to launch a kernel that has set `--threads` to a specific value.\n",
    "Here is a [discourse link](https://github.com/JuliaLang/IJulia.jl/issues/882#issuecomment-579520246) on how to do that.\n",
    "\n",
    "1. Launch an interactive Julia session\n",
    "2. Run the following commands\n",
    "```julia\n",
    "using IJulia\n",
    "IJulia.installkernel(\"Julia 4 Threads\", env=Dict(\n",
    "    \"JULIA_NUM_THREADS\" => \"4\",\n",
    "))\n",
    "```\n",
    "3. Exit Julia `exit()`\n",
    "4. Launch jupyter notebook `jupyter notebook`\n",
    "5. You can now switch your kernel!\n",
    "\n",
    "**Exercise 2**: Do this with the number of cores you have on your machine and launch your multi-threaded Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-running with a new kernel\n",
    "\n",
    "I am going to re-run with a new kernel and see if I have more threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, this number should now be 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration 1: Minimum Distance\n",
    "\n",
    "Let's suppose I have a dataset $x_1 \\dots x_n \\in \\mathbb{R}^d$.\n",
    "\n",
    "- Here is code that will compute the minimum distance across all pairs of vectors.\n",
    "- You should copy this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_min_dist (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_min_dist(dataset; slow=false)\n",
    "    n = length(dataset)\n",
    "    \n",
    "    min_dist = Inf\n",
    "    for i=1:n, j=(i+1):n\n",
    "        dist_val = norm(dataset[i] - dataset[j])\n",
    "        min_dist = min(dist_val, min_dist)\n",
    "    end\n",
    "    \n",
    "    # simulate an intensive computation\n",
    "    if slow\n",
    "        sleep(5)\n",
    "    end\n",
    "    \n",
    "    return min_dist\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just check that this runs on a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8004991222082375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "d = 3\n",
    "\n",
    "# generate dataset\n",
    "dataset = [randn(d) for i=1:n]\n",
    "\n",
    "# compute min dist\n",
    "compute_min_dist(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that I had several datasets.\n",
    "Let's start by making 3 datasets of a larger size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "d = 20\n",
    "\n",
    "# generate 4 datasets\n",
    "dataset1 = [randn(d) for i=1:n]\n",
    "dataset2 = [randn(d) for i=1:n]\n",
    "dataset3 = [randn(d) for i=1:n]\n",
    "dataset4 = [randn(d) for i=1:n];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's time how long it takes to run serial code on all three of these datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.276913 seconds (4.00 M allocations: 426.822 MiB, 23.81% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.302077920504382"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time begin\n",
    "    md1 = compute_min_dist(dataset1)\n",
    "    md2 = compute_min_dist(dataset2)\n",
    "    md3 = compute_min_dist(dataset3)\n",
    "    md4 = compute_min_dist(dataset4)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our serial code above, the minimum distance is computed for the first dataset, then for the second, then for the third.\n",
    "\n",
    "Now let's see how to use multi-threading to have these run in parallel!\n",
    "\n",
    "### Multi-Threading in Action: Syntax\n",
    "\n",
    "We can use the `Threads.@spawn` macro which is as follows:\n",
    "\n",
    "- `t = Threads.@spawn exp` creates a task `t` that will evaluate the expression `exp` using an available thread\n",
    "- In order to retrieve the value given by `exp`, we need to use `fetch(t)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.087556 seconds (4.04 M allocations: 428.862 MiB, 14.49% gc time, 67.83% compilation time)\n"
     ]
    }
   ],
   "source": [
    "@time begin\n",
    "    \n",
    "    # spawning threads\n",
    "    t1 = Threads.@spawn compute_min_dist(dataset1)\n",
    "    t2 = Threads.@spawn compute_min_dist(dataset2)\n",
    "    t3 = Threads.@spawn compute_min_dist(dataset3)\n",
    "    t4 = Threads.@spawn compute_min_dist(dataset4)\n",
    "\n",
    "    # fetching results\n",
    "    p_md1 = fetch(t1)\n",
    "    p_md2 = fetch(t2)\n",
    "    p_md3 = fetch(t3)\n",
    "    p_md4 = fetch(t4)\n",
    "end\n",
    "\n",
    "@assert isapprox(md1, p_md1)\n",
    "@assert isapprox(md2, p_md2)\n",
    "@assert isapprox(md3, p_md3)\n",
    "@assert isapprox(md4, p_md4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "- What is the speed increase? Is it proportioal to the # of threads?\n",
    "- Why or why not? Does `@time` tell us anything helpful here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Threading a `for` Loop\n",
    "\n",
    "You can also use the `Threads` package to automatically run `for` loops with multi-threading.\n",
    "\n",
    "```julia\n",
    "Threads.@threads for i=1:n\n",
    "    x[i] = myfunc(i)\n",
    "end\n",
    "```\n",
    "\n",
    "- this will automatically create tasks to run\n",
    "- there is another syntax we can use, but let's see it later\n",
    "\n",
    "We'll recreate the example above, but with 25 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.320962 seconds (24.98 M allocations: 4.652 GiB, 17.29% gc time, 1.67% compilation time)\n",
      "  0.714050 seconds (25.16 M allocations: 4.661 GiB, 16.59% gc time, 32.76% compilation time)\n"
     ]
    }
   ],
   "source": [
    "num_datasets = 25\n",
    "n = 1000\n",
    "d = 40\n",
    "\n",
    "# slow -- set this true if you want compute_min_dist to be artificially challenging\n",
    "slow = false\n",
    "\n",
    "# generate many datasets\n",
    "datasets = [ [randn(d) for i=1:n] for k=1:num_datasets ]\n",
    "\n",
    "# initialize the array of results\n",
    "md_val = zeros(num_datasets)\n",
    "p_md_val = zeros(num_datasets)\n",
    "\n",
    "# run the serial code\n",
    "@time begin\n",
    "    for k=1:num_datasets\n",
    "       md_val[k] = compute_min_dist(datasets[k]; slow=slow) \n",
    "    end\n",
    "end\n",
    "\n",
    "# run the parallel code\n",
    "@time begin\n",
    "    Threads.@threads for k=1:num_datasets\n",
    "       p_md_val[k] = compute_min_dist(datasets[k]; slow=slow) \n",
    "    end\n",
    "end\n",
    "\n",
    "# check the same answer\n",
    "@assert all(isapprox.(md_val, p_md_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions\n",
    "\n",
    "- What is the speed up now? \n",
    "- Is this close or far from optimal, given the number of threads?\n",
    "- Why is this happening?\n",
    "- What changes if we set `slow = true`? Why?\n",
    "\n",
    "Note: there is nothing to \"fetch\" here because the expression `p_md_vals[k] = compute_min_dist(datasets[k])` does not itself have an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Exercise: Monte Carlo Estimation\n",
    "\n",
    "In this exercise, I want you to use the `Threads` package to paralleize a simple Monte Carlo estimation.\n",
    "\n",
    "- If $(X,Y) \\sim \\textrm{Unif}[0,1]$ then $\\Pr( X^2 + Y^2 \\leq 1 ) = \\frac{\\pi}{4}$ *why?*\n",
    "- So, we can use Monte Carlo sampling (i.e. law of large numbers) to estimate $\\pi$ by the following:\n",
    "  - Sample $\\{X_i, Y_i\\}_{i=1}^n \\sim \\textrm{Unif}[0,1]$\n",
    "  - Compute the estimate $\\hat{p} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i^2 + Y_i^2 \\leq 1\\}$\n",
    "  - Compute an estimate for $\\pi$ via $\\hat{\\pi} = 4 \\hat{p}$\n",
    "\n",
    "The following serial code will run this. Your goal is to parallelize it, and time the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mc_estimate_pi (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mc_estimate_pi(num_samples)\n",
    "    count = 0\n",
    "    for i=1:num_samples\n",
    "        x,y = rand(2)\n",
    "        if x^2 + y^2 <= 1\n",
    "            count += 1\n",
    "        end\n",
    "    end\n",
    "    return 4 * (count / num_samples)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Samples: 10 \t Time: 0.057777025 \t Estimated Pi: 2.8\n",
      "Num Samples: 100 \t Time: 6.079e-6 \t Estimated Pi: 3.24\n",
      "Num Samples: 1000 \t Time: 2.9875e-5 \t Estimated Pi: 3.188\n",
      "Num Samples: 100000 \t Time: 0.003114607 \t Estimated Pi: 3.13384\n",
      "Num Samples: 10000000 \t Time: 0.346879577 \t Estimated Pi: 3.1418496\n",
      "Num Samples: 100000000 \t Time: 3.356312932 \t Estimated Pi: 3.14159376\n"
     ]
    }
   ],
   "source": [
    "num_sample_vec = [10, 100, 1000, 100_000, 10_000_000, 100_000_000]\n",
    "\n",
    "for num_samples in num_sample_vec\n",
    "    elap_time = @elapsed pi_est = mc_estimate_pi(num_samples)\n",
    "    println(\"Num Samples: $num_samples \\t Time: $elap_time \\t Estimated Pi: $pi_est\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn - implement a multi-threaded version of this and time the difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Strategies\n",
    "\n",
    "Unfortunately, class ended right about here and I waved my hands a bit.\n",
    "I want to highlight a few possible strategies that may or may not have worked.\n",
    "\n",
    "### Naive Implementation: Race Conditions\n",
    "\n",
    "The most naive implementation has a race condition.\n",
    "We'll see what that means next (and how to prevent it!), but I just want to highlight that it will result in severely underestimating $\\pi$ *due to memory over-write issues*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Samples: 10 \t Time: 0.109941842 \t Estimated Pi: 3.2\n",
      "Num Samples: 100 \t Time: 7.7727e-5 \t Estimated Pi: 3.12\n",
      "Num Samples: 1000 \t Time: 0.000115161 \t Estimated Pi: 2.184\n",
      "Num Samples: 100000 \t Time: 0.004403535 \t Estimated Pi: 0.98616\n",
      "Num Samples: 10000000 \t Time: 0.45870064 \t Estimated Pi: 0.9367492\n",
      "Num Samples: 100000000 \t Time: 4.918535244 \t Estimated Pi: 0.8907236\n"
     ]
    }
   ],
   "source": [
    "function multi_mc_estimate_pi_bad1(num_samples)\n",
    "    count = 0\n",
    "    Threads.@threads for i=1:num_samples\n",
    "        x,y = rand(2)\n",
    "        if x^2 + y^2 <= 1\n",
    "            count += 1 # race condition ! overwriting shared memory\n",
    "        end\n",
    "    end\n",
    "    return 4 * (count / num_samples)\n",
    "end\n",
    "\n",
    "num_sample_vec = [10, 100, 1000, 100_000, 10_000_000, 100_000_000]\n",
    "\n",
    "for num_samples in num_sample_vec\n",
    "    elap_time = @elapsed pi_est = multi_mc_estimate_pi_bad1(num_samples)\n",
    "    println(\"Num Samples: $num_samples \\t Time: $elap_time \\t Estimated Pi: $pi_est\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows us that we are severely underestimating $\\pi$. See more below on what is a race condition and why this is happening: `hint`: closely examine the shared variable `count`\n",
    "\n",
    "### Safe Implementation\n",
    "\n",
    "Here's another implementation which avoids the race conditions:\n",
    "\n",
    "- Create a shared array `events` for the 0/1 event of whether $X^2 + Y^2 \\leq 1$\n",
    "- Each task writes to the shared array in parallel (no race conditions)\n",
    "- We sum the array at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Samples: 10 \t Time: 0.142836314 \t Estimated Pi: 2.4\n",
      "Num Samples: 100 \t Time: 6.7688e-5 \t Estimated Pi: 3.2\n",
      "Num Samples: 1000 \t Time: 9.252e-5 \t Estimated Pi: 3.172\n",
      "Num Samples: 100000 \t Time: 0.005108917 \t Estimated Pi: 3.14108\n",
      "Num Samples: 10000000 \t Time: 0.268595659 \t Estimated Pi: 3.1408504\n",
      "Num Samples: 100000000 \t Time: 2.169292747 \t Estimated Pi: 3.14180504\n"
     ]
    }
   ],
   "source": [
    "function multi_mc_estimate_pi_bad2(num_samples)\n",
    "    event_array = zeros(num_samples)\n",
    "    Threads.@threads for i=1:num_samples\n",
    "        x,y = rand(2)\n",
    "        event_array[i] = x^2 + y^2 <= 1\n",
    "    end\n",
    "    return 4 * (sum(event_array) / num_samples)\n",
    "end\n",
    "\n",
    "num_sample_vec = [10, 100, 1000, 100_000, 10_000_000, 100_000_000]\n",
    "\n",
    "for num_samples in num_sample_vec\n",
    "    elap_time = @elapsed pi_est = multi_mc_estimate_pi_bad2(num_samples)\n",
    "    println(\"Num Samples: $num_samples \\t Time: $elap_time \\t Estimated Pi: $pi_est\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we observed\n",
    "\n",
    "- We've fixed the race condition, so the estimated $\\pi$ is correct.\n",
    "- While we are seeing a little bit of speed-up here for larger `num_samples`, it is not very significant.\n",
    "\n",
    "The reason we don't have super significant speed up is that these computational tasks aren't that demanding relative to the start-up cost.\n",
    "\n",
    "- **Exercise**: Does increasing `num_samples` yield a better speed up? What computational costs might this have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfalls of Multi-Threading: Race Conditions\n",
    "\n",
    "Let's demonstrate a common pitfall of multi-threading known as *race conditions*.\n",
    "\n",
    "Let's start by seeing how we might parallelize a sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sum_single (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sum_single(a)\n",
    "    sum_val = 0\n",
    "    for x in a\n",
    "        sum_val += x\n",
    "    end\n",
    "    return sum_val\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = collect(1:100)\n",
    "sum_single(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, great - how let's try to just apply the `Threads.@threads` macro to parallelize this loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sum_multi_bad (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sum_multi_bad(a)\n",
    "    sum_val = 0\n",
    "    Threads.@threads for x in a\n",
    "        sum_val += x\n",
    "    end\n",
    "    return sum_val\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4791"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = collect(1:100)\n",
    "sum_multi_bad(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is not returning the right answer! And what's worse is that it returns a different answer every time I run it! **What the heck!**\n",
    "\n",
    "The **problem** is that we're abusing the *shared memory*\n",
    "\n",
    "- Each thread has read / write access to the variable `sum_val`\n",
    "- Thread A goes to read `sum_val` and then computes `y = sum_val + x`\n",
    "- At the same time, Thread B reads `sum_val` and then computes `y' = sum_val + x'`\n",
    "- Then Thread A writes `sum_val = y`\n",
    "- Then Thread B overwrites `sum_val = y'`\n",
    "- And it's as if the addition of `x` didn't even happen.\n",
    "\n",
    "This is called a *race condition*. It was the reason that x-ray machine was killing people.\n",
    "\n",
    "We have two options to avoid race conditions:\n",
    "\n",
    "- **Lock Mechanisms**: We could place additional \"locks\" on the variable `sum_val` so race conditions don't occur. But, this kind of defeats the purpose of parallelism here.\n",
    "- **Separate Data Buffers**: We could split the data up, have each thread work on it's own data, and then merge the answers together again.\n",
    "\n",
    "We're going to focus on the second approach. \n",
    "But to do this, we'll need to take a detour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Map and Iterators\n",
    "\n",
    "Let me show off a few things about Julia here, which will make implementing separate data buffers much easier.\n",
    "\n",
    "### The `map` function\n",
    "\n",
    "The `map` function exists is many functional programming languages.\n",
    "It has two inputs `map(f, x)` which applies the function `f` to every element in `x` and returns the answer.\n",
    "This is, by definition, *embarassingly parallel*.\n",
    "Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 1.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 4, 9, 16]\n",
    "\n",
    "map(sqrt, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, the function `f` you want to apply isn't pre-made. \n",
    "So, you have to make it. \n",
    "You can write an *anonymous function* like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Int64}:\n",
       "  1\n",
       "  4\n",
       "  9\n",
       " 16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 2, 3, 4]\n",
    "\n",
    "map(t->t^2, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read this like \"the function `f` takes in `t` and outputs `t^2`\".\n",
    "This allows you to write your own functions here without explicitly naming them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       "  4.158529015192103\n",
       " 11.090702573174319\n",
       " 20.85887999194013\n",
       " 32.75680249530793"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 2, 3, 4]\n",
    "\n",
    "map(t->t^2 + 4*t - sin(t), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But writing this notation can be really annoying and awkward, especially if your function `f` naturally has multiple lines.\n",
    "What do you do in this case?\n",
    "Well, there is an alternative specification for `map`.\n",
    "\n",
    "It looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " 1.0\n",
       " 1.4142135623730951\n",
       " 1.7320508075688772\n",
       " 2.0\n",
       " 2.23606797749979"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4,5]\n",
    "\n",
    "y = map(x) do t\n",
    "    return sqrt(t)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the same style of `map` call but with a more complex `f`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Vector{Int64}:\n",
       "   4\n",
       "  16\n",
       "  28\n",
       "   6\n",
       " 130\n",
       "  88"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 5, 9, 12, 43, 29]\n",
    "\n",
    "y = map(x) do t\n",
    "    # collatz conjecture\n",
    "    if iseven(t)\n",
    "        return div(t,2)\n",
    "    else\n",
    "        return 3 * t + 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `map` calls are *embarassingly parallel*, which makes them especially good for multi-threading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another detour: reduce and mapreduce\n",
    "\n",
    "The `reduce` function is a cousin to `map`.\n",
    "\n",
    "```julia\n",
    "reduce(op, itr; init)\n",
    "\n",
    "```\n",
    "- apply the binary operation `op` successively to elements in iterable `itr`\n",
    "- the optional argument `init` is the initial value\n",
    "\n",
    "So, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4,5]\n",
    "\n",
    "reduce(+, x; init=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(*, x; init=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **very common** and **very parallelizable** type of operation is to combine `map` and `reduce`.\n",
    "This is called `mapreduce`.\n",
    "\n",
    "```julia\n",
    "mapreduce(f, op, itr; init)\n",
    "\n",
    "```\n",
    "- compute `x = map(f, itr)`\n",
    "- return `y = reduce(op, x; init)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4,5]\n",
    "\n",
    "mapreduce(x->x^2, +, x; init=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have time, I will show you how `mapreduce` can be implemented in Distributed Memory model of parallel programming using Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final detour: partition iterator\n",
    "\n",
    "The last thing I want to show you is an iterator construction.\n",
    "There is a fancy way to iterate over partitions of an array of a fixed size.\n",
    "\n",
    "It's easier to just show you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[4, 5, 6]\n",
      "[7, 8, 9]\n",
      "[10]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# this is an iterator, it doesn't create new memory\n",
    "chunks = Iterators.partition(a, 3)\n",
    "\n",
    "# I am going to iterate now\n",
    "for chunk in chunks\n",
    "    println(chunk)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one last thing, we can use the function `cld` to compute `ceil( x / y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cld(100, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be helpful for computing one chunk for each of our threads!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Multi-threading: Data Buffer\n",
    "\n",
    "So now let's see how to use `Iterations.partition` and `map` together to create a data buffer for multi-tasking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sum_multi_good (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sum_multi_good(a)\n",
    "   chunks = Iterators.partition(a, cld(length(a), Threads.nthreads()))\n",
    "   tasks = map(chunks) do chunk\n",
    "       Threads.@spawn sum_single(chunk)\n",
    "   end\n",
    "   chunk_sums = fetch.(tasks)\n",
    "   return sum(chunk_sums)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Break this code down line-by-line and explain it\n",
    "\n",
    "1. What is the code doing?\n",
    "2. How does this avoid the race condition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = collect(1:100)\n",
    "\n",
    "sum_multi_good(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay - it's working!\n",
    "\n",
    "But let us make a sobering remark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000017 seconds (1 allocation: 16 bytes)\n",
      "  0.000203 seconds (39 allocations: 2.266 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time sum(a)\n",
    "@time sum_multi_good(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overhead here for parallelization was not worth it.\n",
    "\n",
    "- The built-in `sum` function is super fast\n",
    "- Re-writing a fast function and parallelizing it is typically a waste of time\n",
    "- You only want to parallelize over functions that take a long time to compute\n",
    "\n",
    "So, this example really served to demonstrate race conditions.\n",
    "Trying to re-implement `sum` is not advisable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "Let me make a few remarks:\n",
    "\n",
    "- When you are multi-threading (or coding anything, really) -- **don't re-invent the wheel**\n",
    "- You can often check to see if someone has a solution for the problem you have (not on HW though)\n",
    "- For example, `OhMyThreads.jl` has higher level support for threaded mapreduce and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 4 Threads 1.12",
   "language": "julia",
   "name": "julia-4-threads-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

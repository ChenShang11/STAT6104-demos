{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia Performance Tips\n",
    "\n",
    "In this notebook, I will give performance tips for programming in Julia.\n",
    "We will see specific examples in action.\n",
    "\n",
    "For more examples and detailed explanations, read the fantastic advice in the links below\n",
    "\n",
    "- Julia Documentation: Performance Tips [link](https://docs.julialang.org/en/v1/manual/performance-tips/)\n",
    "- Chris Rackauckas SiML Course: Optimizing Serial Code [link](https://book.sciml.ai/notes/02-Optimizing_Serial_Code/)\n",
    "- Modern Julia Workflows: Optimizng Tutorial [link](https://modernjuliaworkflows.org/optimizing/)\n",
    "- Guillaume Dalle: High Performance Julia [link](https://gdalle.github.io/JuliaPerf-CERMICS/)\n",
    "\n",
    "Please follow along using your own machine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Write Efficient Julia Code\n",
    "\n",
    "How should you learn to write efficient Julia code?\n",
    "By talking with and learning from others!\n",
    "The Julia community is very welcoming and helpful:\n",
    "\n",
    "- Discourse [link](https://discourse.julialang.org/)\n",
    "- StackOverflow [link](https://stackoverflow.com/questions/tagged/julia?tab=Newest)\n",
    "\n",
    "After a bit of time using Julia, you may also want to answer questions on these sites.\n",
    "By answering questions, you figure out what makes a good question and also you learn more about the language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Performance Tips\n",
    "\n",
    "There are a few general performance tips when writing in Julia.\n",
    "\n",
    "1. Do not use global variables\n",
    "2. Put your code into functions\n",
    "3. Facilitate type inference\n",
    "4. Reduce memory allocations\n",
    "\n",
    "If you're in the habit of writing modular code, then (1) and (2) are already taken care of.\n",
    "Honestly, a lot of Julia documentation focusing on facilitating type inference but I haven't run into an example where this is the issue in my research code; perhaps that's because I'm lucky enough to be writing type-stable functions (doubtful).\n",
    "\n",
    "From my own experience using Julia, unnecessary memory allocations is always the culprit.\n",
    "For these reason, reducing memory allocations will be the main focus of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing Code\n",
    "\n",
    "In order to show off these performance tips, we will need to measure the performance of our code. \n",
    "So, how do you time your code in Julia?\n",
    "\n",
    "There are two options:\n",
    "\n",
    "1. **Option 1**: `@time exp`\n",
    "  - executes expression `exp`, reports execution time and memory allocation\n",
    "  - buil-in utility macro\n",
    "2. **Option 2**: `@btime exp`\n",
    "  - like `@time`, but runs the expression multiple times for higher reporting accuracy\n",
    "  - (*requires the `BenchmarkTools.jl` package*)\n",
    "  \n",
    "Let's see how to use `@time` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.014414 seconds (114 allocations: 89.297 KiB, 71.67% compilation time)\n"
     ]
    }
   ],
   "source": [
    "A = randn(100, 100)\n",
    "B = randn(100, 100)\n",
    "\n",
    "@time A * B;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the execution time and the number of allocations.\n",
    "But let's see what happens when we run the function again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000277 seconds (3 allocations: 116.078 KiB)\n"
     ]
    }
   ],
   "source": [
    "@time A * B;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoah - the execution time dropped by a lot.\n",
    "What's up with that?\n",
    "\n",
    "Julia is a Just-In-Time (JIT) compiled language.\n",
    "This means that the first time a function is called within a Julia session, the source code is compiled down to optimized machine code.\n",
    "It will then be faster every time it is run during the session.\n",
    "\n",
    "In fact, note that the `@time` macro actually warned us about this, by stating the percent compile time.\n",
    "\n",
    "Let's just see this happen again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First run:    0.020761 seconds (1.45 k allocations: 146.406 KiB, 97.63% compilation time)\n",
      "Second run:   0.000732 seconds (3 allocations: 80.078 KiB)\n"
     ]
    }
   ],
   "source": [
    "function mat_mul(A,B)\n",
    "    return A * B\n",
    "end\n",
    "\n",
    "n = 100\n",
    "A = randn(n,n)\n",
    "B = randn(n,n)\n",
    "\n",
    "print(\"First run:  \")\n",
    "@time mat_mul(A,B)\n",
    "\n",
    "print(\"Second run: \")\n",
    "@time mat_mul(A,B);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@time` macro is great, but it has a few drawbacks.\n",
    "The main drawback is that the expression is only run once, leading to sometimes unstable reporting of execution time.\n",
    "The reason is that you machine is doing many things, not just running Julia.\n",
    "So, the execution timing can differ between runs.\n",
    "\n",
    "To overcome this issue (and the pre-compile issue) you might want to use `@btime`.\n",
    "It evaluates the expression multiple times and reports median info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  153.627 μs (3 allocations: 80.08 KiB)\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "@btime mat_mul(A,B);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, `@btime` is just a wrapper for `@benchmark`, which in another timing macro that gives you a lot more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation per sample.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m155.578 μs\u001b[22m\u001b[39m … \u001b[35m 24.148 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m234.258 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m280.622 μs\u001b[22m\u001b[39m ± \u001b[32m373.489 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m5.09% ± 7.62%\n",
       "\n",
       "  \u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m█\u001b[34m▇\u001b[39m\u001b[39m▅\u001b[32m▄\u001b[39m\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\n",
       "  \u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▃\u001b[39m▃\u001b[39m▄\u001b[39m▅\u001b[39m▁\u001b[39m▃\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▁\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▄\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▅\u001b[39m \u001b[39m█\n",
       "  156 μs\u001b[90m        \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m        1.4 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m80.08 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m3\u001b[39m."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark mat_mul(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see more summary statistics about our execution time from 1,000 evaluations of the expression.\n",
    "Pretty cool!\n",
    "\n",
    "In fact, there are a lot of bells and whistles that `@benchmark` comes with.\n",
    "For example, suppose you wanted to test the speed of `mat_mul` on many random matrices `A` and `B`.\n",
    "You might write something like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation per sample.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m216.829 μs\u001b[22m\u001b[39m … \u001b[35m 15.060 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m 0.00% … 97.40%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m395.794 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m451.038 μs\u001b[22m\u001b[39m ± \u001b[32m426.610 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m10.97% ± 11.58%\n",
       "\n",
       "  \u001b[39m▅\u001b[39m▂\u001b[39m▁\u001b[39m▆\u001b[34m█\u001b[39m\u001b[39m▅\u001b[32m▄\u001b[39m\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\n",
       "  \u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▆\u001b[39m▅\u001b[39m▄\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▅\u001b[39m▅\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▇\u001b[39m \u001b[39m█\n",
       "  217 μs\u001b[90m        \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m       2.69 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m240.23 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m9\u001b[39m."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100\n",
    "@benchmark mat_mul(randn(n,n), randn(n,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this has a downside: the generation of the two random matrices `A` and `B` is included in the run time and the memory estimates!\n",
    "In fact, we can see this because the execution time is noticeably larger than in the example above.\n",
    "\n",
    "How can we get around this?\n",
    "\n",
    "Well, `@benchmark` has a functionality specifically for this.\n",
    "We can use the `setup` arguement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation per sample.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m158.648 μs\u001b[22m\u001b[39m … \u001b[35m 13.254 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 97.56%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m228.091 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m243.729 μs\u001b[22m\u001b[39m ± \u001b[32m226.185 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m5.81% ±  6.86%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\u001b[39m▇\u001b[39m▇\u001b[39m▄\u001b[34m▅\u001b[39m\u001b[39m█\u001b[39m▃\u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▃\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m▅\u001b[39m\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m▂\n",
       "  159 μs\u001b[90m           Histogram: frequency by time\u001b[39m          395 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m80.08 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m3\u001b[39m."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark mat_mul(data...) setup=(data=(randn(n,n), randn(n,n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyways, we can go down this rabbit hole of timing basically forever.\n",
    "The reality is that, for our purposes of optimizing our code, we don't care about minor improvements in run time, we can about significant (say, 2x or 10x) speed ups.\n",
    "So these details of how exactly you ought to time the code won't matter so much.\n",
    "It will typically be fine to just use `@btime` by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This exercise is to be completed in small groups of 3 or less*\n",
    "\n",
    "**Exercise 1**: Benchmark a function for matrix conjugation, i.e. compute $A^\\top X A$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Memory Allocations\n",
    "\n",
    "Now that we have a way to evaluate the performance of our code, we can move onto one of the most important lessons in Julia -- and indeed, in any high level programming language: reducing memory allocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Place Functions and Pre-Allocating Memory\n",
    "\n",
    "The first way to reduce memory allocations is to not allocate memory at all.\n",
    "\n",
    "- An **in-place function** is one which modifies its input arguments directly (i.e. overwrites) rather than creating and returning a new modified object.\n",
    "- In Julia, in-place functions have the convention of ending with a `!`\n",
    "\n",
    "Let's see a simple example based on sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " -0.6104295279403974\n",
       " -0.08816670911839933\n",
       " -0.2796911726064586\n",
       "  0.7880772986789002\n",
       "  0.613527674395732"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is y [-0.6104295279403974, -0.2796911726064586, -0.08816670911839933, 0.613527674395732, 0.7880772986789002]\n",
      "This is x [-0.6104295279403974, -0.08816670911839933, -0.2796911726064586, 0.7880772986789002, 0.613527674395732]\n"
     ]
    }
   ],
   "source": [
    "y = sort(x) # this is not in-place, because it creates a new array y\n",
    "\n",
    "println(\"This is y $y\")\n",
    "println(\"This is x $x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is x [-0.6104295279403974, -0.2796911726064586, -0.08816670911839933, 0.613527674395732, 0.7880772986789002]\n"
     ]
    }
   ],
   "source": [
    "sort!(x) # this is in place\n",
    "\n",
    "println(\"This is x $x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the difference in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort (copy):   2.940 ms (12 allocations: 1.17 MiB)\n",
      "Sort (in-place):  205.397 μs (0 allocations: 0 bytes)\n"
     ]
    }
   ],
   "source": [
    "x = randn(10^5)\n",
    "\n",
    "print(\"Sort (copy): \")\n",
    "@btime sort(x)\n",
    "\n",
    "print(\"Sort (in-place):\")\n",
    "@btime sort!(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a small example about adding new elements to an array.\n",
    "\n",
    "- `vcat` will create a new array\n",
    "- `append!` will update the second array in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vcat:    1.281 μs (5 allocations: 8.12 KiB)\n",
      "append!:   64.722 ns (0 allocations: 0 bytes)\n"
     ]
    }
   ],
   "source": [
    "x = randn(1000)\n",
    "a = 5\n",
    "\n",
    "print(\"vcat:  \")\n",
    "@btime vcat(x, [a]) # this allocates a new array \n",
    "\n",
    "print(\"append!: \")\n",
    "@btime append!(x, a); # this performs the action in-place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a difference of a factor of 20x -- that can be a huge improvement if this gets called in your code millions of times.\n",
    "This would be a huge improvement in, say, a Monte Carlo simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at an in-place matrix-vector multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard   1.498 μs (2 allocations: 928 bytes)\n",
      "In Place   1.395 μs (0 allocations: 0 bytes)\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "n = 100\n",
    "\n",
    "y = zeros(n)\n",
    "A = randn(n,n)\n",
    "x = randn(n)\n",
    "\n",
    "print(\"Standard \")\n",
    "@btime y = A * x\n",
    "\n",
    "print(\"In Place \")\n",
    "@btime mul!(y,A,x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvement here is not impressive.\n",
    "That is probably because the dominating cost is the actual linear algebra, rather than the memory allocation.\n",
    "\n",
    "**Note**: You can use existing in-place functions, or make them yourself\n",
    "\n",
    "- To find existing in-place implementations of a function, just search it online\n",
    "- You might want to make your own functions in-place *if allocation is the bottleneck*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This exercise is to be completed in small groups of 3 or less*\n",
    "\n",
    "**Exercise 2**: Benchmark the difference between `filter` and `filter!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Views Instead of Slices\n",
    "\n",
    "Another way that you can accidentally use memory allocation is to use slices, rather than views.\n",
    "\n",
    "- Slicing an array `a[3:5]` allocates a new array in memory\n",
    "- Using a view `@view a[3:5]` does not allocate new memory\n",
    "\n",
    "Let's see an example of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  553.531 ns (4 allocations: 4.08 KiB)\n",
      "  163.395 ns (2 allocations: 64 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-7.5933451079071785"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = randn(1000)\n",
    "\n",
    "@btime sum(a[5:500])\n",
    "@btime sum(@view a[5:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the macro `@views` outside of the entire evaluation too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.5933451079071785"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@views sum(a[5:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fused Vectorizations\n",
    "\n",
    "Julia has a special **dot syntax** which converts a scalar function into a function that acts element-wise on arrays.\n",
    "This can help the compiler to optimize the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Int64}:\n",
       "  1\n",
       "  4\n",
       "  9\n",
       " 16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4]\n",
    "\n",
    "x.^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful from a practical point of view: it may be easier to write in this dot synatx than to code the full loop.\n",
    "\n",
    "But, there is a secondary purpose here.\n",
    "\n",
    "Dot syntax is *fusing*, which means that they are combined at the syntax level into a single loop, without allocating temporary arrays.\n",
    "\n",
    "Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  537.496 μs (32 allocations: 4.59 MiB)\n",
      "  74.506 μs (22 allocations: 784.78 KiB)\n"
     ]
    }
   ],
   "source": [
    "x = randn(10^5);\n",
    "\n",
    "@btime 3 * x.^2 + 4 * x + 7 * x.^3 # this is *not* fused  --\n",
    "@btime 3 .* x.^2 .+ 4 .* x .+ 7 .* x.^3; # this is fused "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing in dot syntax can get us great speed-ups, but it can also be ugly.\n",
    "You can use the `@.` macro to convert every function call or operator into a \"dot\" call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  74.755 μs (21 allocations: 784.72 KiB)\n"
     ]
    }
   ],
   "source": [
    "@btime @. 3*x^2 + 4*x + 7*x^3; # equivalent to above, but prettier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the number of allocations above is slighly misleading. \n",
    "If we just wrap this in functions, we can see much smaller allocations.\n",
    "\n",
    "Let this be a warning against taking benchmarking *too seriously*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  524.861 μs (18 allocations: 4.59 MiB)\n",
      "  68.474 μs (3 allocations: 784.06 KiB)\n"
     ]
    }
   ],
   "source": [
    "f(x) = 3 * x.^2 + 4 * x + 7 * x.^3\n",
    "fdot(x) = @. 3*x^2 + 4*x + 7*x^3\n",
    "\n",
    "@btime f(x)\n",
    "@btime fdot(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just see another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.347 ms (12 allocations: 3.06 MiB)\n",
      "  1.254 ms (3 allocations: 784.06 KiB)\n"
     ]
    }
   ],
   "source": [
    "f1(x) = exp.(x) + exp.(-x) # the + is not fused!\n",
    "f2(x) = @. exp(x) + exp(-x)\n",
    "\n",
    "@btime f1(x)\n",
    "@btime f2(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance gain here is much more modest, but you can see the reduction in allocation happening here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This exercise is to be completed in small groups of 3 or less*\n",
    "\n",
    "**Exercise 3**: Create two equivalent functions that apply $x^2 + \\sqrt{x} - 5$ to each entry of a vector, where one function uses fused vectorization and the other function does not. Benchmark the difference on a relevant test case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use StaticArrays.jl for small vectors / matrices\n",
    "\n",
    "If your function uses many small arrays (e.g. length `<= 100`) and the size of these arrays before execution, then you can use the `StaticArrays.jl` which offers much faster memory allocation over the `Array` types in base Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Addition\n",
      "  107.163 ns (2 allocations: 144 bytes)\n",
      "  66.188 ns (1 allocation: 80 bytes)\n",
      "Matrix Multiplication\n",
      "  146.122 ns (2 allocations: 144 bytes)\n",
      "  77.499 ns (1 allocation: 80 bytes)\n"
     ]
    }
   ],
   "source": [
    "using StaticArrays\n",
    "\n",
    "A_s = @SMatrix [1 3 5 ; 2 3 6; 9 2 3]\n",
    "B_s = @SMatrix [4 5 9 ; 5 4 4; 9 7 5]\n",
    "\n",
    "A_n = [1 3 5 ; 2 3 6; 9 2 3]\n",
    "B_n = [4 5 9 ; 5 4 4; 9 7 5]\n",
    "\n",
    "println(\"Matrix Addition\")\n",
    "@btime A_n + B_n\n",
    "@btime A_s + B_s\n",
    "\n",
    "println(\"Matrix Multiplication\")\n",
    "@btime A_n * B_n\n",
    "@btime A_s * B_s;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant\n",
      "  388.772 ns (5 allocations: 240 bytes)\n",
      "  49.392 ns (1 allocation: 16 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Determinant\")\n",
    "@btime det(A_n)\n",
    "@btime det(A_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, that speed up is pretty nice, almost a factor of 10x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This exercise is to be completed in small groups of 3 or less*\n",
    "\n",
    "**Exercise 4**: Write a function to compute the 4th power of a matrix. Benchmark the performance difference between `Array` and `StaticArray` on a 4-by-4 example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
